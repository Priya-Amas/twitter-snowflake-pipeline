# ğŸ¦ Twitter to Snowflake Data Pipeline

This is an end-to-end real-world data pipeline that:

1. Fetches tweets from Twitter API (v2)
2. Saves them as CSV
3. Uploads to Snowflake via Python
4. Orchestrates with Apache Airflow

---

## ğŸš€ Technologies Used

- Python ğŸ
- Snowflake â„ï¸
- Airflow ğŸ›«
- Git & GitHub
- Bash (Ubuntu WSL)

---

## ğŸ“ Project Structure

(venv) priya@LAPTOP-J0SSV9L1:~/projects$ tree -L 3
.
â””â”€â”€ snowflake_twitter_pipeline
    â”œâ”€â”€ airflow
    â”‚Â Â  â”œâ”€â”€ airflow.cfg
    â”‚Â Â  â”œâ”€â”€ airflow.db
    â”‚Â Â  â”œâ”€â”€ dags
    â”‚Â Â  â”œâ”€â”€ logs
    â”‚Â Â  â””â”€â”€ webserver_config.py
    â”œâ”€â”€ cached_tweets.json
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ sql
    â”‚Â Â  â”œâ”€â”€ tweets_20250616_203428.csv
    â”‚Â Â  â”œâ”€â”€ tweets_20250616_203731.csv
    â”œâ”€â”€ src
    â”‚Â Â  â”œâ”€â”€ __init__.py
    â”‚Â Â  â”œâ”€â”€ __pycache__
    â”‚Â Â  â”œâ”€â”€ load_to_snowflake.py
    â”‚Â Â  â””â”€â”€ twitter_to_csv.py
    â”œâ”€â”€ tests
    â”‚Â Â  â”œâ”€â”€ __pycache__
    â”‚Â Â  â””â”€â”€ test_twitter_to_csv.py
    â””â”€â”€ venv
        â”œâ”€â”€ bin
        â”œâ”€â”€ include
        â”œâ”€â”€ lib
        â”œâ”€â”€ lib64 -> lib
        â”œâ”€â”€ pyvenv.cfg
        â””â”€â”€ share



---

## ğŸ› ï¸ How to Run

1. Clone the repo:
   ```bash
   git clone https://github.com/<your-username>/snowflake_twitter_pipeline.git
   cd snowflake_twitter_pipeline

2.Create and activate virtualenv:

bash:

python3 -m venv venv
source venv/bin/activate

3.Install requirements:

bash:

pip install -r requirements.txt
Set your environment variables in .env file.

Run Airflow:

bash:

airflow db migrate
airflow scheduler
airflow webserver

âœ… DAG Tasks
fetch_tweets: Gets tweets using Twitter API and saves as CSV

load_to_snowflake: Uploads CSV to Snowflake using Python connector

ğŸ”’ Security
Secrets like API keys and credentials are stored in .env and excluded from Git using .gitignore.

---


ğŸ‘©â€ğŸ’» Author
Priya A
Snowflake Data Engineer
